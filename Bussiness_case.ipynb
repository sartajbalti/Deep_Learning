{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction model \n",
    "A prediction model for an Audiobook to predict the customer's response to buy the book again or not.\n",
    "\n",
    "- Prepare the Dataset \n",
    "    - Balance the Dataset \n",
    "    - Divide the Dataset\n",
    "    - Save data into a tensor \n",
    "- Create a Machine Leaning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\sartaj\\anaconda3\\envs\\agmn\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sartaj\\anaconda3\\envs\\agmn\\lib\\site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sartaj\\anaconda3\\envs\\agmn\\lib\\site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\sartaj\\anaconda3\\envs\\agmn\\lib\\site-packages (from scikit-learn->sklearn) (1.21.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\sartaj\\anaconda3\\envs\\agmn\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\sartaj\\anaconda3\\envs\\agmn\\lib\\site-packages (from scikit-learn->sklearn) (1.7.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imort Libraries \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessi \n",
    "data = np.loadtxt('Audiobooks_data.csv', delimiter=',')\n",
    "uscale_data = data[:,1:-1]\n",
    "target_all = data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing the dataset\n",
    "num_ones = int(np.sum(target_all))\n",
    "zero_target = 0\n",
    "indicies_to_remove = []\n",
    "for i in range (target_all.shape[0]):\n",
    "    if target_all[i]==0:\n",
    "        zero_target += 1\n",
    "        if zero_target > num_ones:\n",
    "            indicies_to_remove.append(i)\n",
    "unsacled_inputs_equal_priors = np.delete(uscale_data, indicies_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(target_all, indicies_to_remove, axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize the Input \n",
    "scale_inputs = preprocessing.scale(unsacled_inputs_equal_priors)\n",
    "scale_targets = preprocessing.scale(targets_equal_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the Dataset \n",
    "shuffled_indicies = np.arange(scale_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indicies) \n",
    "shuffled_inputs = scale_inputs[shuffled_indicies]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indicies]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data percent:  79.99552972731337 %\n",
      "Validation data percent:  9.991059454626733 %\n",
      "Test data percent:  10.013410818059901 %\n"
     ]
    }
   ],
   "source": [
    "# Split into train , Validation and Test\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "train_samples = int(samples_count*0.8)\n",
    "validation_samples = int(samples_count*0.1)\n",
    "test_samples = samples_count - train_samples - validation_samples\n",
    "train_inputs = shuffled_inputs[:train_samples]\n",
    "train_targets = shuffled_targets[:train_samples]\n",
    "validation_inputs  = shuffled_inputs[train_samples:train_samples + validation_samples]\n",
    "validation_targets = shuffled_targets[train_samples:train_samples + validation_samples]\n",
    "test_inputs = shuffled_inputs[train_samples + validation_samples:]\n",
    "test_targets = shuffled_targets[train_samples + validation_samples:]\n",
    "print(\"Training data percent: \", np.sum(train_samples)*100/np.sum(samples_count),\"%\")\n",
    "print(\"Validation data percent: \", np.sum(validation_samples)*100/np.sum(samples_count),\"%\")\n",
    "print(\"Test data percent: \",np.sum(test_samples)*100/np.sum(samples_count),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train.npz', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_val.npz', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test.npz', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('agmn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ae8e7103e2551143fb447a9b6084dfd31386d5c03ebb907114bf53983988c95"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
